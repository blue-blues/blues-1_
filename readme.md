# to do

1 . using new flash atttention model 3.0 from git

2. implemnation of deepspeed
   
3. creating an new dataset for indian

4. traning an tokenzier custom - (optional)
   
5. creating an new translation transformer for the translation
   
7. traning to create an good pretrain model


# completed!
1. just flash attention with working model - 5/11/24




#traning
#6/11/24 - 13:58 - pid #9358
dataset - 1065519731 at 1024
